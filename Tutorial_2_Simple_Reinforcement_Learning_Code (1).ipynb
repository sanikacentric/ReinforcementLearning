{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDsQZMP8iQbu"
      },
      "source": [
        "# Simple Reinforcement Learning Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgDLVZ-3iQbw"
      },
      "source": [
        "# The anatomy of the agent\n",
        "\n",
        "The entities in RL's world are,\n",
        "\n",
        "\n",
        "- The agent Class: A thing, or person, that tries to gain rewards by interaction. In practice, the agent is a piece of code that implements some policy\n",
        "\n",
        "- The environment Class: It's a model of the world that is external to the agent.It provides observations and rewards to agent.\n",
        "\n",
        "With this basic understanding, let's try to implement these to classes..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5da6MfFiQbw"
      },
      "source": [
        "## Our first RL Code\n",
        "\n",
        "To make the things very simple, let's create a dummy environment that gives the agent some random rewards everytime, regardless of the\n",
        "agent's actions.\n",
        "\n",
        "Though this is not of any practical usage, it allow us\n",
        "to focus on implementation of environment and agent\n",
        "classes.\n",
        "\n",
        "Our enviornment class should be capable of handling actions received from the agent. This is done by **action** method, which checks the number of steps left and returns a random reward, by ignoring the agent's action\n",
        "\n",
        "_______init___ constructor is called to set the number of episodes for the event, get_observation() method is supposed to return the current environment's observation to the agent, but in this case returns a zero vector.\n",
        "\n",
        "Other methods are mostly self explanatory, get_actions returns 0 or 1 corresponding to two available actions.is_done checks the end of episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB13VVM2iQbw"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import List\n",
        "\n",
        "class SampleEnvironment:\n",
        "    def __init__(self):\n",
        "        self.steps_left = 20\n",
        "\n",
        "    def get_observation(self) -> List[float]:\n",
        "        return [0.0, 0.0, 0.0]\n",
        "\n",
        "    def get_actions(self) -> List[int]:\n",
        "        return [0, 1]\n",
        "\n",
        "    def is_done(self) -> bool:\n",
        "        return self.steps_left == 0\n",
        "\n",
        "    def action(self, action: int) -> float:\n",
        "        if self.is_done():\n",
        "            raise Exception(\"Game is over\")\n",
        "        self.steps_left -= 1\n",
        "        return random.random()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKdj6ODUiQbx"
      },
      "source": [
        "The agent's Class simple and includes only two methods: the constructor and the method that performs one step in the environment\n",
        "\n",
        "Intitially the total reward collected is set to zero by the constructor.\n",
        "\n",
        "The step function accepts environment instance as an argument\n",
        "and allows agent to perform the following actions:\n",
        "- Observe the environment\n",
        "- Make a decision about the action to take based on the observations\n",
        "- Submit the action to the environment\n",
        "- Get the reward for the current step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNTHwxoPiQbx",
        "outputId": "0b0ae5a6-fdce-46b2-fd5f-3144334527eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random.choice([0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5F7LjaBiQbx"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def step(self, env: SampleEnvironment):\n",
        "        current_obs = env.get_observation()\n",
        "        print(\"Observation {}\".format(current_obs))\n",
        "        actions = env.get_actions()\n",
        "        print(actions)\n",
        "        reward = env.action(random.choice(actions))\n",
        "        self.total_reward += reward\n",
        "        print(\"Total Reward {}\".format(self.total_reward))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qTRgwiSiQby",
        "outputId": "e0a14395-d6fe-4d24-951c-c05fb8a58f71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Steps 1\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 0.004761513978586485\n",
            "Steps 2\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 0.029422781438137147\n",
            "Steps 3\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 0.46886987036685057\n",
            "Steps 4\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 0.8630454254661049\n",
            "Steps 5\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 1.381528652094357\n",
            "Steps 6\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 1.45411069286047\n",
            "Steps 7\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 1.8556611431092866\n",
            "Steps 8\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 2.1321480492739777\n",
            "Steps 9\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 3.1080873410706777\n",
            "Steps 10\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 3.4470307497808106\n",
            "Steps 11\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 3.901810986962796\n",
            "Steps 12\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 4.012043274552123\n",
            "Steps 13\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 4.023768740000734\n",
            "Steps 14\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 4.807652436945311\n",
            "Steps 15\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 5.767110643044976\n",
            "Steps 16\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 6.578319129324807\n",
            "Steps 17\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 6.899766492601371\n",
            "Steps 18\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 7.057941539511659\n",
            "Steps 19\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 7.61161897560968\n",
            "Steps 20\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 8.288088414657892\n",
            "Total reward got: 8.2881\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env = SampleEnvironment()\n",
        "    agent = Agent()\n",
        "    i=0\n",
        "\n",
        "    while not env.is_done():\n",
        "        i=i+1\n",
        "        print(\"Steps {}\".format(i))\n",
        "        agent.step(env)\n",
        "\n",
        "    print(\"Total reward got: %.4f\" % agent.total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F6NwAtdiQby"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}